"""
Chat + RAG Component for Okloa.

This component combines the Chat interface with RAG functionality,
providing a question-answer interface that uses Colbert RAG to retrieve
relevant emails and then processes them through an LLM for comprehensive answers.

Enhanced with agentic components that determine:
1. Whether RAG is needed for a given question
2. The optimal 'k' value for document retrieval
"""

import streamlit as st
import pandas as pd
import os
import sys
import time
from typing import List, Dict, Any, Tuple, Optional

# Add the necessary paths
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Import required modules from the project
from src.rag.colbert_rag import search_with_colbert, format_result_preview
from src.llm.openrouter import openrouter_llm_api_call
from src.llm.agents import RAGOrchestrator, get_rag_parameters
from constants import ACTIVE_PROJECT

# Check if RAGAtouille is available
RAGATOUILLE_AVAILABLE = True
try:
    from src.rag.colbert_initialization import initialize_colbert_rag_system
except ImportError as e:
    print(f"RAGAtouille import failed: {e}")
    RAGATOUILLE_AVAILABLE = False


def create_professional_prompt(user_question: str, retrieved_emails: List[Dict[str, Any]]) -> str:
    """
    Create a professional prompt for the LLM based on user question and retrieved emails.
    
    Args:
        user_question: The user's original question
        retrieved_emails: List of emails retrieved by RAG
    
    Returns:
        Complete prompt for the LLM
    """
    system_prompt = """Vous √™tes un assistant professionnel sp√©cialis√© dans l'analyse d'archives d'emails. 
Votre r√¥le est de r√©pondre aux questions des utilisateurs en vous basant UNIQUEMENT sur les informations 
pr√©sentes dans la collection d'emails fournie.

INSTRUCTIONS IMPORTANTES:
1. R√©pondez UNIQUEMENT √† partir des informations pr√©sentes dans les emails fournis
2. Si les informations ne sont pas suffisantes pour r√©pondre, indiquez-le clairement
3. Citez les emails pertinents en mentionnant l'exp√©diteur et le sujet quand possible
4. Soyez pr√©cis et factuel
5. N'inventez aucune information qui ne serait pas dans les emails
6. Structurez votre r√©ponse de mani√®re claire et professionnelle"""

    # Construct the email context
    emails_context = "\n\n=== EMAILS R√âCUP√âR√âS ===\n"
    
    for i, email in enumerate(retrieved_emails, 1):
        metadata = email.get('metadata', {})
        text_content = email.get('text', '')
        
        emails_context += f"\n--- EMAIL {i} ---\n"
        emails_context += f"De: {metadata.get('from', 'Inconnu')}\n"
        emails_context += f"√Ä: {metadata.get('to_recipients', 'Inconnu')}\n"
        emails_context += f"Sujet: {metadata.get('subject', 'Pas de sujet')}\n"
        emails_context += f"Date: {metadata.get('date', 'Date inconnue')}\n"
        emails_context += f"Contenu:\n{text_content}\n"
    
    user_prompt = f"""Question de l'utilisateur: {user_question}

{emails_context}

=== CONSIGNE ===
En vous basant UNIQUEMENT sur les emails ci-dessus, r√©pondez √† la question de l'utilisateur. 
Si les informations ne permettent pas de r√©pondre compl√®tement, indiquez-le clairement."""

    return system_prompt, user_prompt


def render_chat_rag_component(emails_df: pd.DataFrame):
    """
    Render the Chat + RAG component with agentic capabilities.
    
    Args:
        emails_df: DataFrame containing email data
    """
    
    # Check if RAGAtouille is available
    if not RAGATOUILLE_AVAILABLE:
        st.error("RAGAtouille n'est pas install√© ou introuvable dans l'environnement Python actuel.")
        st.info("""
        Pour utiliser Chat + RAG, vous devez installer la biblioth√®que RAGAtouille.

        Veuillez ex√©cuter ces commandes dans votre terminal:
        ```
        pip install ragatouille
        pip install -r requirements.txt
        ```

        Assurez-vous de l'installer dans le m√™me environnement Python que votre application Streamlit.
        Apr√®s l'installation, red√©marrez votre application Streamlit.
        """)
        return

    # Page title and description
    st.title("Chat + RAG - Assistant IA pour vos emails")
    
    st.markdown("""
    Cette interface combine le chat conversationnel avec la recherche RAG avanc√©e et des **agents intelligents**.
    Posez vos questions et obtenez des r√©ponses bas√©es sur vos archives d'emails.
    
    **Comment √ßa fonctionne:**
    1. ü§ñ **Agents IA** : Analysent votre question pour d√©terminer si RAG est n√©cessaire et combien d'emails r√©cup√©rer
    2. üîç **Recherche RAG** : Trouve les emails pertinents dans vos archives (si n√©cessaire)
    3. üß† **LLM** : Analyse les emails trouv√©s et formule une r√©ponse compl√®te
    4. üìß **Sources** : Vous voyez la r√©ponse et pouvez consulter les emails sources
    """)

    # Set up paths for the indexes
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    path_to_metadata = os.path.join(project_root, 'data', 'Projects', ACTIVE_PROJECT, 'colbert_indexes')
    ragatouille_index_path = os.path.join(project_root, '.ragatouille', 'colbert', 'indexes', f'{ACTIVE_PROJECT}_emails_index')

    # Check if index exists
    index_exists = os.path.exists(os.path.join(path_to_metadata, 'email_metadata.pkl'))

    if not index_exists:
        st.warning("‚ö†Ô∏è L'index ColBERT n'a pas encore √©t√© cr√©√©.")
        st.info("Veuillez d'abord cr√©er l'index dans la page 'Colbert RAG' avant d'utiliser Chat + RAG.")
        return

    # Configuration section in sidebar
    st.sidebar.title("‚öôÔ∏è Configuration")
    
    # Agent configuration
    st.sidebar.subheader("ü§ñ Agents IA")
    use_agents = st.sidebar.checkbox(
        "Utiliser les agents intelligents",
        value=True,
        help="Les agents d√©terminent automatiquement si RAG est n√©cessaire et le nombre optimal d'emails √† r√©cup√©rer"
    )
    
    # RAG parameters
    st.sidebar.subheader("Param√®tres RAG")
    if use_agents:
        st.sidebar.info("üß† Les agents d√©termineront automatiquement le nombre d'emails √† r√©cup√©rer")
        max_emails = st.sidebar.slider(
            "Nombre maximum d'emails",
            min_value=5,
            max_value=20,
            value=15,
            step=1,
            help="Limite maximale pour les agents"
        )
    else:
        num_emails = st.sidebar.slider(
            "Nombre d'emails √† r√©cup√©rer",
            min_value=3,
            max_value=15,
            value=10,
            step=1,
            help="Nombre d'emails pertinents √† r√©cup√©rer pour r√©pondre √† la question"
        )
    
    # LLM parameters
    st.sidebar.subheader("Param√®tres LLM")
    model_options = [
        "openai/gpt-4o",
        "openai/gpt-4o-mini", 
        "anthropic/claude-3.5-sonnet",
        "anthropic/claude-3-haiku",
        "meta-llama/llama-3.1-8b-instruct"
    ]
    selected_model = st.sidebar.selectbox(
        "Mod√®le LLM",
        options=model_options,
        index=1,  # Default to gpt-4o-mini for agents
        help="Mod√®le de langage √† utiliser pour les agents et la g√©n√©ration de r√©ponses"
    )

    # Initialize conversation history in session state
    if "chat_rag_history" not in st.session_state:
        st.session_state.chat_rag_history = []

    # Display conversation history
    st.subheader("üí¨ Conversation")
    
    for message in st.session_state.chat_rag_history:
        if message["role"] == "user":
            st.chat_message("user").write(message["content"])
        else:
            with st.chat_message("assistant"):
                st.write(message["content"])
                
                # Display agent decision if available
                if "metadata" in message and "agent_decision" in message["metadata"]:
                    agent_data = message["metadata"]["agent_decision"]
                    with st.expander("ü§ñ D√©cision des agents"):
                        st.write(f"**RAG n√©cessaire:** {'Oui' if agent_data['needs_rag'] else 'Non'}")
                        if agent_data['needs_rag']:
                            st.write(f"**K s√©lectionn√©:** {agent_data['k_value']}")
                        st.write(f"**Raisonnement:** {agent_data['reasoning']}")
                        st.write(f"**Confiance:** {agent_data['confidence']:.2f}")
                
                # Display sources if available
                if "sources" in message and message["sources"]:
                    with st.expander("üìß Voir les emails sources"):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**Email {i}:**")
                            st.markdown(source)
                            st.markdown("---")

    # Chat input
    user_question = st.chat_input("Posez votre question sur vos emails...")

    if user_question:
        # Display user message
        st.chat_message("user").write(user_question)
        
        # Add to history
        st.session_state.chat_rag_history.append({
            "role": "user", 
            "content": user_question
        })

        # Process the question
        with st.chat_message("assistant"):
            # Show processing steps
            status_placeholder = st.empty()
            
            try:
                start_time = time.time()
                
                # Step 0: Agent Decision (if enabled)
                agent_decision_time = 0
                final_k = num_emails if not use_agents else max_emails
                rag_params = None
                
                if use_agents:
                    status_placeholder.info("ü§ñ Agents IA: Analyse de la question...")
                    print(f"ü§ñ Agent > RAG > LLM: Analyzing question: '{user_question}'")
                    
                    agent_start = time.time()
                    rag_params = get_rag_parameters(
                        user_question=user_question,
                        model=selected_model,
                        max_k=max_emails
                    )
                    agent_decision_time = time.time() - agent_start
                    
                    print(f"ü§ñ Agent decision: {rag_params}")
                    
                    if not rag_params["needs_rag"]:
                        # Question doesn't need RAG - answer directly with LLM
                        status_placeholder.info("ü§ñ Les agents ont d√©termin√© que cette question ne n√©cessite pas de recherche RAG")
                        print("ü§ñ Agent > LLM: Direct LLM response (no RAG needed)")
                        
                        llm_start_time = time.time()
                        llm_response = openrouter_llm_api_call(
                            system_prompt="Vous √™tes un assistant IA utile et informatif. R√©pondez de mani√®re claire et pr√©cise.",
                            user_prompt=user_question,
                            model=selected_model
                        )
                        llm_time = time.time() - llm_start_time
                        
                        # Clear status and show response
                        status_placeholder.empty()
                        
                        st.write(llm_response)
                        
                        # Show agent decision info
                        with st.expander("ü§ñ D√©cision des agents"):
                            st.write(f"**RAG n√©cessaire:** Non")
                            st.write(f"**Raisonnement:** {rag_params['reasoning']}")
                            st.write(f"**Confiance:** {rag_params['confidence']:.2f}")
                        
                        total_time = time.time() - start_time
                        st.caption(f"‚è±Ô∏è Temps total: {total_time:.2f}s (Agents: {agent_decision_time:.2f}s, LLM: {llm_time:.2f}s)")
                        
                        print(f"ü§ñ Agent > LLM: Complete. Total time: {total_time:.2f}s")
                        
                        # Add to conversation history
                        st.session_state.chat_rag_history.append({
                            "role": "assistant",
                            "content": llm_response,
                            "sources": [],
                            "metadata": {
                                "agent_decision": rag_params,
                                "used_rag": False,
                                "agent_time": agent_decision_time,
                                "llm_time": llm_time,
                                "total_time": total_time,
                                "model": selected_model
                            }
                        })
                        return
                    
                    # RAG is needed - use agent-determined k value
                    final_k = rag_params["k_value"]
                    status_placeholder.info(f"ü§ñ Agents: RAG requis avec k={final_k} (confiance: {rag_params['confidence']:.2f})")
                    print(f"ü§ñ Agent > RAG > LLM: RAG needed with k={final_k}")
                    time.sleep(0.5)  # Brief pause to show agent decision
                
                # Step 1: RAG Search
                status_placeholder.info(f"üîç Recherche d'emails pertinents (k={final_k})...")
                print(f"üîç RAG search: Searching with k={final_k}")
                
                search_start = time.time()
                retrieved_emails = search_with_colbert(
                    query=user_question,
                    path_to_metadata=path_to_metadata,
                    ragatouille_index_path=ragatouille_index_path,
                    top_k=final_k
                )
                search_time = time.time() - search_start
                
                if not retrieved_emails:
                    status_placeholder.warning("‚ö†Ô∏è Aucun email pertinent trouv√©")
                    response = "Je n'ai pas trouv√© d'emails pertinents pour r√©pondre √† votre question."
                    st.write(response)
                    print("‚ö†Ô∏è No relevant emails found")
                    
                    # Add to history
                    st.session_state.chat_rag_history.append({
                        "role": "assistant",
                        "content": response,
                        "sources": [],
                        "metadata": {
                            "agent_decision": rag_params,
                            "used_rag": True,
                            "agent_time": agent_decision_time if use_agents else 0,
                            "search_time": search_time,
                            "num_sources": 0,
                            "model": selected_model
                        }
                    })
                    return

                status_placeholder.info(f"‚úÖ {len(retrieved_emails)} emails trouv√©s ({search_time:.2f}s)")
                print(f"‚úÖ RAG search: Found {len(retrieved_emails)} emails in {search_time:.2f}s")
                
                # Step 2: LLM Processing
                status_placeholder.info("üß† G√©n√©ration de la r√©ponse par IA...")
                print("üß† LLM processing: Generating response...")
                
                # Create the prompt
                system_prompt, user_prompt = create_professional_prompt(user_question, retrieved_emails)
                
                # Call LLM
                llm_start_time = time.time()
                llm_response = openrouter_llm_api_call(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    model=selected_model
                )
                llm_time = time.time() - llm_start_time
                
                # Clear status and show final response
                status_placeholder.empty()
                
                # Display the response
                st.write(llm_response)
                
                # Show timing information with agent details
                total_time = time.time() - start_time
                
                if use_agents:
                    st.caption(f"‚è±Ô∏è Temps total: {total_time:.2f}s (Agents: {agent_decision_time:.2f}s, Recherche: {search_time:.2f}s, LLM: {llm_time:.2f}s)")
                    
                    # Show agent decision details
                    with st.expander("ü§ñ D√©cision des agents"):
                        if rag_params:
                            st.write(f"**RAG n√©cessaire:** Oui")
                            st.write(f"**K s√©lectionn√©:** {rag_params['k_value']}")
                            st.write(f"**Raisonnement:** {rag_params['reasoning']}")
                            st.write(f"**Confiance:** {rag_params['confidence']:.2f}")
                        else:
                            st.write("Erreur dans la d√©cision des agents")
                else:
                    st.caption(f"‚è±Ô∏è Temps total: {total_time:.2f}s (Recherche: {search_time:.2f}s, LLM: {llm_time:.2f}s)")
                
                print(f"üß† LLM processing: Complete. Total time: {total_time:.2f}s")
                
                # Prepare source previews
                source_previews = [format_result_preview(email) for email in retrieved_emails]
                
                # Display sources in expandable section
                with st.expander(f"üìß Voir les {len(retrieved_emails)} emails sources"):
                    for i, preview in enumerate(source_previews, 1):
                        st.markdown(f"**Email {i}:**")
                        st.markdown(preview)
                        if i < len(source_previews):
                            st.markdown("---")
                
                # Add to conversation history
                st.session_state.chat_rag_history.append({
                    "role": "assistant",
                    "content": llm_response,
                    "sources": source_previews,
                    "metadata": {
                        "agent_decision": rag_params,
                        "used_rag": True,
                        "agent_time": agent_decision_time if use_agents else 0,
                        "search_time": search_time,
                        "llm_time": llm_time,
                        "total_time": total_time,
                        "num_sources": len(retrieved_emails),
                        "model": selected_model
                    }
                })

            except Exception as e:
                status_placeholder.error(f"‚ùå Erreur: {str(e)}")
                error_response = f"Je rencontre une erreur lors du traitement de votre question: {str(e)}"
                st.write(error_response)
                print(f"‚ùå Error in chat processing: {str(e)}")
                
                # Add error to history
                st.session_state.chat_rag_history.append({
                    "role": "assistant",
                    "content": error_response,
                    "sources": []
                })

    # Action buttons
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üóëÔ∏è Effacer la conversation"):
            st.session_state.chat_rag_history = []
            st.rerun()
    
    with col2:
        if st.session_state.chat_rag_history:
            if st.button("üìã Exporter la conversation"):
                # Create export content
                export_content = "# Conversation Chat + RAG avec Agents\n\n"
                for i, message in enumerate(st.session_state.chat_rag_history):
                    if message["role"] == "user":
                        export_content += f"**Utilisateur:** {message['content']}\n\n"
                    else:
                        export_content += f"**Assistant:** {message['content']}\n\n"
                        
                        # Add agent decision info
                        if "metadata" in message and "agent_decision" in message["metadata"]:
                            agent_data = message["metadata"]["agent_decision"]
                            export_content += f"**D√©cision des agents:**\n"
                            export_content += f"- RAG n√©cessaire: {'Oui' if agent_data['needs_rag'] else 'Non'}\n"
                            if agent_data['needs_rag']:
                                export_content += f"- K s√©lectionn√©: {agent_data['k_value']}\n"
                            export_content += f"- Confiance: {agent_data['confidence']:.2f}\n"
                            export_content += f"- Raisonnement: {agent_data['reasoning']}\n\n"
                        
                        if "sources" in message and message["sources"]:
                            export_content += "**Sources:**\n"
                            for j, source in enumerate(message["sources"], 1):
                                export_content += f"Email {j}:\n{source}\n\n"
                        export_content += "---\n\n"
                
                st.download_button(
                    label="üíæ T√©l√©charger",
                    data=export_content,
                    file_name=f"conversation_chat_rag_agents_{int(time.time())}.md",
                    mime="text/markdown"
                )
    
    with col3:
        if st.button("‚ÑπÔ∏è Aide"):
            st.info("""
            **Conseils d'utilisation:**
            
            ‚Ä¢ Posez des questions sp√©cifiques sur le contenu de vos emails
            ‚Ä¢ Utilisez des termes cl√©s pertinents pour am√©liorer la recherche
            ‚Ä¢ Les agents d√©terminent automatiquement si RAG est n√©cessaire
            ‚Ä¢ Vous pouvez demander des r√©sum√©s, des dates, des personnes mentionn√©es
            
            **Exemples de questions:**
            ‚Ä¢ "Quand est pr√©vue la prochaine r√©union ?" (RAG probable)
            ‚Ä¢ "Qui a envoy√© des informations sur le projet X ?" (RAG probable)
            ‚Ä¢ "Comment √©crire un bon email ?" (LLM direct probable)
            ‚Ä¢ "R√©sume-moi les emails de Marie Durand" (RAG avec k √©lev√©)
            """)

    # System status in sidebar
    st.sidebar.markdown("---")
    st.sidebar.subheader("üìä Statut du syst√®me")
    
    # Agent status
    if use_agents:
        st.sidebar.success("‚úÖ Agents IA activ√©s")
    else:
        st.sidebar.info("üîß Mode manuel")
    
    # Index status
    if index_exists:
        st.sidebar.success("‚úÖ Index ColBERT pr√™t")
        
        # Show index stats if available
        try:
            import pickle
            metadata_path = os.path.join(path_to_metadata, "email_metadata.pkl")
            if os.path.exists(metadata_path):
                with open(metadata_path, "rb") as f:
                    email_metadata = pickle.load(f)
                st.sidebar.info(f"üìß {len(email_metadata)} emails index√©s")
        except:
            pass
    else:
        st.sidebar.error("‚ùå Index non cr√©√©")
    
    # Conversation stats
    if st.session_state.chat_rag_history:
        user_messages = [m for m in st.session_state.chat_rag_history if m["role"] == "user"]
        rag_used = len([m for m in st.session_state.chat_rag_history 
                       if m["role"] == "assistant" and m.get("metadata", {}).get("used_rag", False)])
        st.sidebar.info(f"üí¨ {len(user_messages)} questions pos√©es")
        st.sidebar.info(f"üîç {rag_used} utilisations de RAG")
